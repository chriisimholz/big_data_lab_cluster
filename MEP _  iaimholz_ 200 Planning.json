{"paragraphs":[{"text":"%md\n# Planning","user":"bd01","dateUpdated":"2021-05-30T09:18:20+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Planning</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1620063661846_-1400616422","id":"20210127-071031_1944670894","dateCreated":"2021-05-03T17:41:01+0000","dateStarted":"2021-05-30T09:18:20+0000","dateFinished":"2021-05-30T09:18:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:55"},{"text":"%md\n\n### Für mein Projekt plane ich folgende Schritte:\n\n- Herunterladen von JQ auf GW zur Auswertung der Metadaten im JSON-Format\n- Metadaten mit JQ und curl anzeigen\n- Daten mit API/curl beziehen \n- Mit Pyspark Daten aus CSV in Dataframe lesen und Schema definieren\n- Auswertungen und Abfragen mit Pyspark und SQL\n- Grundsätzlich würde sich ein ETL Prozess anbieten, da die Messstationen laufend Daten liefern und auch diese bereinigt werden müssen. Je nach Zeitaufwand werde ich dies noch zusätzlich vornehmen. Priorisiert werden zu Beginn vorerst andere Aufgaben.","user":"bd01","dateUpdated":"2021-05-30T13:50:30+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Für mein Projekt plane ich folgende Schritte:</h3>\n<ul>\n  <li>Herunterladen von JQ auf GW zur Auswertung der Metadaten im JSON-Format</li>\n  <li>Metadaten mit JQ und curl anzeigen</li>\n  <li>Daten mit API/curl beziehen</li>\n  <li>Mit Pyspark Daten aus CSV in Dataframe lesen und Schema definieren</li>\n  <li>Auswertungen und Abfragen mit Pyspark und SQL</li>\n  <li>Grundsätzlich würde sich ein ETL Prozess anbieten, da die Messstationen laufend Daten liefern und auch diese bereinigt werden müssen. Je nach Zeitaufwand werde ich dies noch zusätzlich vornehmen. Priorisiert werden zu Beginn vorerst andere Aufgaben.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1620221951827_-1712366727","id":"20210505-133911_162598573","dateCreated":"2021-05-05T13:39:11+0000","dateStarted":"2021-05-30T13:50:30+0000","dateFinished":"2021-05-30T13:50:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"text":"%md\r\n### Überraschungen oder spezielle Challenges:\r\n#### Generell:\r\n- Namenskonvention von Anfang berücksichtigen\r\n\r\n#### Bezug der Daten per API:\r\n- JQ war bereits auf dem Gateway und musste auch nicht aktualisiert werden (sudo apt install jq).\r\n- Die Daten werden zuerst auf Gateway gespeichert. -> Spätere Erkenntnis: Kann auch direkt auf Datanodes abgespeichert werden.\r\n\r\n#### DB:\r\n- Probleme mit dem Attribut \"date\" bei SQL-Abfragen. Ich habe dieses danach auf \"timestampdate\" geändert.\r\n- Durch das Modul/Projekt konnte ich meine SQL Kenntnisse wieder auffrischen. \r\n\r\n#### Chartdarstellung von Analysis 1:\r\n- Anpassung der Configs des Zeppelin Notebooks in Ambari (export ZEPPELIN_INTERPRETER_OUTPUT_LIMIT=10000000, export ZEPPELIN_MEM=-Xmx4g)\r\n- Anpassung des Spark2 Intepreter im Zeppelin Notebook (zeppelin.spark.maxResult)\r\n- Anpassung von RAM(lxc config set cl-hpelb1-50-gw-01-lx-ub18 limits.memory 10GB)\r\n\r\n#### Chartdarstellung von Analysis 2:\r\n- keine\r\n\r\n#### Chartdarstellung von Analysis 3:\r\n- keine","user":"bd01","dateUpdated":"2021-05-30T13:52:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Überraschungen oder spezielle Challenges:</h3>\n<h4>Generell:</h4>\n<ul>\n  <li>Namenskonvention von Anfang berücksichtigen</li>\n</ul>\n<h4>Bezug der Daten per API:</h4>\n<ul>\n  <li>JQ war bereits auf dem Gateway und musste auch nicht aktualisiert werden (sudo apt install jq).</li>\n  <li>Die Daten werden zuerst auf Gateway gespeichert. -&gt; Spätere Erkenntnis: Kann auch direkt auf Datanodes abgespeichert werden.</li>\n</ul>\n<h4>DB:</h4>\n<ul>\n  <li>Probleme mit dem Attribut &ldquo;date&rdquo; bei SQL-Abfragen. Ich habe dieses danach auf &ldquo;timestampdate&rdquo; geändert.</li>\n  <li>Durch das Modul/Projekt konnte ich meine SQL Kenntnisse wieder auffrischen.</li>\n</ul>\n<h4>Chartdarstellung von Analysis 1:</h4>\n<ul>\n  <li>Anpassung der Configs des Zeppelin Notebooks in Ambari (export ZEPPELIN_INTERPRETER_OUTPUT_LIMIT=10000000, export ZEPPELIN_MEM=-Xmx4g)</li>\n  <li>Anpassung des Spark2 Intepreter im Zeppelin Notebook (zeppelin.spark.maxResult)</li>\n  <li>Anpassung von RAM(lxc config set cl-hpelb1-50-gw-01-lx-ub18 limits.memory 10GB)</li>\n</ul>\n<h4>Chartdarstellung von Analysis 2:</h4>\n<ul>\n  <li>keine</li>\n</ul>\n<h4>Chartdarstellung von Analysis 3:</h4>\n<ul>\n  <li>keine</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1621676548439_1743837439","id":"20210522-094228_115373411","dateCreated":"2021-05-22T09:42:28+0000","dateStarted":"2021-05-30T13:52:44+0000","dateFinished":"2021-05-30T13:52:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"text":"%md\n","user":"bd01","dateUpdated":"2021-05-30T09:18:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1622365339231_1212489876","id":"20210530-090219_1723736701","dateCreated":"2021-05-30T09:02:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:58"}],"name":"MEP /  iaimholz/ 200 Planning","id":"2G4QK4S6U","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}